\documentclass[12pt]{article}
\usepackage{paralist,amsmath,amssymb,thumbpdf,lmodern}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{physics}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage[outdir=./]{epstopdf}
\usepackage{caption}
\usepackage{bm}
\usepackage{makecell}
\usepackage{ifxetex,ifluatex}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\begingroup\expandafter\expandafter\expandafter\endgroup
\expandafter\ifx\csname IncludeInRelease\endcsname\relax
  \usepackage{fixltx2e}
\fi
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\usepackage{newtxtext,newtxmath}

\title{Biostat 882 HW1}
\date{}
\author{Mukai Wang 98830336}


\usepackage{Sweave}
\begin{document}
\maketitle


\emph{Note: Check the electronic version of the homework submission to have access to C++ and R code on Github.}

\section*{Problem 1}

Given a number $c>0$, I can construct a function
\[\nu_1(\diff X_{n+1}) = \frac{1}{\sqrt{2\pi}} \exp (-\frac{X_{n+1}^2+c^2}{2})\diff X_{n+1} \]

for $X_{n+1} \in  \mathbb{R}_+$ such that $K(X_n, X_{n+1}) > \nu_1(\diff X_{n+1})$,
 $\forall X_{n+1} \in \mathbb{R}_{+}$ and $X_n \in (0, c)$. This is because
 $$K(X_n, X_{n+1}) = \begin{cases}
 	\int_{-\infty}^{X_n}\frac{1}{\sqrt{2\pi}} \exp(-x^2 /2)\diff x \qquad X_{n+1} = 0\\
 	\frac{1}{\sqrt{2\pi}} \exp(-(X_{n+1}-X_n)^2/2)\diff X_{n+1}\qquad X_{n+1} > 0
 \end{cases}
  $$
  
  When $X_{n+1} > 0$, 
  \[ \frac{\nu_1(\diff X_{n+1})}{K(X_n, X_{n+1})} = \exp(-X_{n+1}X_n - c^2/2 + X_{n}^2/2) < 1\]
  Thus $(0, c)$ is a small set for any $c>0$.
\section*{Problem 2}
 $\{X_n \}_{n=0}^{\infty}$ is not irreducible because for $X_n=0$ all the following chain will have to be zero, i.e. $X_{n+1} = X_{n+2} =\cdots =0$
 Thus $K^n (0, A) = 0 \quad \forall A\subset (0, +\infty)$. Therefore $\{X_n \}_{n=0}^{\infty}$ is not irreducible.

\section*{Problem 3}

\begin{enumerate}[(a)]
	\item According to definition, $X_{n+1} - X_{n} = \delta_n \eta_n + (1-\delta_n) \epsilon_n$ and all of $\delta_n$, $\eta_n$ and $\epsilon_n$ are generated independently of current state value. This defines the Markov chain to be a random walk.
	
	\item The algorithm implementation is detailed in pseudocode \ref{RWMC}. The C++ implementation can be found on \href{https://github.com/mkbwang/BIOSTAT882/blob/master/HW1/P3.cpp}{Github}.
	\begin{algorithm}
		\caption{Random Walk Markov Chain}\label{RWMC}
		\hspace*{\algorithmicindent} \textbf{Input} $X_0$
		\begin{algorithmic}[1]
			\For{$j \gets 1$ to $ 10000$}
			\State $\delta_{j-1} \sim \text{Bernoulli}(0.5)$
			\If{$\delta_{j-1} = 0$}
			\State $X_j \sim N(X_{j-1}, 1)$
			\Else
			\State $\eta_{j-1} \sim \text{Bernoulli}(0.5)$
			\State $X_j \gets X_{j-1} + 2\eta_{j-1} - 1$
			\EndIf
			\EndFor
		\end{algorithmic}
		\hspace*{\algorithmicindent} \textbf{Output} $\{X_1 , X_2 ,\cdots ,X_{10000} \}$
	\end{algorithm}

	A realization is shown in Figure \ref{rwmcchain}.
	\begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.7]{P3mcchain.eps}
		\caption{A realization of Random Walk Markov Chain}\label{rwmcchain}
	\end{figure}
	\item 
	
	The algorithm for estimating $m$ step transitional probability distribution is depicted in pseudocode \ref{mstepest}. The C++ implementation can be found on \href{https://github.com/mkbwang/BIOSTAT882/blob/master/HW1/P3.cpp}{Github}. A realization of three step transition probability distribution is plotted in Figure \ref{mstepplot}.
	\begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.7]{mstep.eps}
		\caption{Three step transition probability distributions from $X=4$}\label{mstepplot}
	\end{figure}
\end{enumerate}



\begin{algorithm}
	\caption{Estimate M Step Transition}\label{mstepest}
	\hspace*{\algorithmicindent} \textbf{Input} Sample Chain $\{Y_0, Y_1, \cdots Y_n \}$, Starting value $x$, Step Size $m$
	\begin{algorithmic}[1]
		\For{$j \gets 0$ to $n-m$}
		\State $X_j \gets x + Y_{j+m} - Y_{j}$
		\EndFor
	\end{algorithmic}
	\hspace*{\algorithmicindent} \textbf{Output} $\{ X_1,X_2, \cdots X_{n-m} \}$
\end{algorithm}


\section*{Problem 4}

\begin{enumerate}[(a)]
	\item The independent Metropolis Hastings algorithm is depicted in pseudocode \ref{IMH}. I run 50 independent Markov chains with 10000 iterations based on each proposal distribution ($\exp(0.1)$ and $\exp(50)$). Then I estimate the rates of the exponential distribution based on the sample draws of the last 1000 iterations for each Markov chain. The C++ \href{https://github.com/mkbwang/BIOSTAT882/blob/master/HW1/P4.cpp}{code} for running the Markov chain and the R \href{https://github.com/mkbwang/BIOSTAT882/blob/master/HW1/P4.R}{code} illustrating the use of it are available on Github. 
	
	\begin{algorithm}
		\caption{Independent Metropolis-Hastings}\label{IMH}
		\hspace*{\algorithmicindent} \textbf{Input} Number of Chains $K$, Chain Length $N$, Target Rate $R_{t}$, Proposal Rate $R_{p}$
		\begin{algorithmic}[1]
			\For{$i \gets 1$ to $K$}
			\State $X_{i,0} \sim \text{Uniform}(R_t/2, 2R_t)$
			\For{$j \gets 0$ to $N-1$}
			\State $Y \sim \exp (R_p)$
			\State $U \sim \text{Uniform}(0, 1)$
			\If{$U < \min \left(1, \exp(-YR_t - X_{i,j}R_p + X_{i,j}R_t+YR_p)\right)$}
			\State $X_{i,j+1}\gets Y$
			\Else
			\State $X_{i,j+1}\gets X_{i,j}$
			\EndIf
			\EndFor
			\EndFor
		\end{algorithmic}
		\hspace*{\algorithmicindent} \textbf{Output} $\{ X_{i,j}\}$, $i=1,2,\cdots K$, $j=1,2,\cdots N$
	\end{algorithm}
	
	
	Based on the boxplots in Figure \ref{IMHperformance}, I can tell that the proposal distribution of $\exp(0.1)$ does a much better job than $\exp(5)$ at converging to the target distribution of $\exp(1)$.
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.7]{IMHestimate.eps}
		\caption{Estimated rate of posterior exponential distribution based on two different proposal distribution $\exp(0.1)$ and $\exp(5)$. The $\exp(0.1)$ does a much better job at convergence to the target distribution of $\exp(1)$.}\label{IMHperformance}
	\end{figure}

	\item We learnt from lecture 3 that the criteria to judge if a proposal distribution $g(x)$ can generate a uniformly ergodic Markov chain for target distribution $f(x)$ is whether
	\[ f(x) \leq Mg(x), \qquad \forall x \in \{ y\in \mathcal{X}: f(y)>0 \} \]
	Now we have the target distribution $f(x) = \exp(-x)$ and two proposal distributions $g_1(x) = 0.1\exp(-0.1x)$ and $g_2(x)=5\exp(-5x)$. We can easily tell that 
	\[ f(x)\leq 10g_1(x)\quad \forall x\geq 0 \]
	and 
	\[ \forall M>0, \exists x>\log(5M/4)\quad \text{such that}\quad f(x) > Mg_2(x) \]
	This proves why $\exp(0.1)$ is a better distribution than $\exp(5)$ for generating a uniformly ergodic chain with posterior distribution of $\exp(1)$.
\end{enumerate}



\section*{Problem 5}


 For $\bm{\mu} = \begin{bmatrix}\mu_1 & \mu_2\end{bmatrix}^\top$, We have 
 \[ \pi (\bm{\mu}) = \frac{1}{\sqrt{2\pi}} \exp\left( -\mu_1^2/2\right)\cdot \frac{1}{\sqrt{0.002\pi}}\exp\left( -500\mu_2^2\right)\cdot \prod_{j=1}^{100}\frac{1}{\sqrt{2\pi}} \exp \left(-(Y_j - \mu_1 - \mu_2)^2 / 2 \right)  \] 
Therefore
\[ \nabla \log \pi(\bm{\mu}) = \begin{pmatrix}
	-\mu_1 + \sum_{j=1}^{100}(Y_j - \mu_1 - \mu_2) \\
	-1000 \mu_2 + \sum_{j=1}^{100}(Y_j - \mu_1 - \mu_2)
 \end{pmatrix} \]

Based on the derivations above, we can set up the Langevin diffusion in algorithm \ref{langevin} with scaling factor tuning. 

\begin{algorithm}
	\caption{Langevin Diffusion}\label{langevin}
	\hspace*{\algorithmicindent} \textbf{Input}  $\{Y_1, Y_2, \cdots, Y_n \}$, Chain Length $N$, initial scaling factor $\sigma_0^2$, scaling factor update period $T$ 
	\begin{algorithmic}[1]
		\State $\bm{\mu}_0 \gets \begin{bmatrix}5 & 5\end{bmatrix}^\top$
		\State $\sigma^2 \gets \sigma_0^2$
		\State $A \gets 0$ \Comment{Counter of acceptance in recent iterations}
		\For{$i\gets 0 $ to $ N-1$}
		\State $Z\sim N\left(\begin{pmatrix}0 \\ 0\end{pmatrix}, \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix} \right)$
		\State $\bm{\mu}' \gets \bm{\mu}_i + \sigma^2 \nabla \log \pi(\bm{\mu}_i)/2 + \sigma Z$
		\State $U\sim \text{Uniform}(0, 1)$
		\If{$U < \min \left(1, \frac{\pi(\bm{\mu}') \cdot \exp (-\lVert\bm{\mu}_i-\bm{\mu}'-\sigma^2\nabla \log \pi(\bm{\mu}')/2 \rVert^2 /2\sigma^2) }{\pi(\bm{\mu}_i) \cdot \exp (-\lVert\bm{\mu}'-\bm{\mu}_i-\sigma^2\nabla \log \pi(\bm{\mu}_i)/2 \rVert^2 / 2\sigma^2)}\right)$}
		\State $A\gets A+1$
		\State $\bm{\mu}_{i+1}\gets \bm{\mu}'$
		\Else
		\State $\bm{\mu}_{i+1}\gets \bm{\mu}_{i}$
		\EndIf
		
		\If{ $i+1$ mod $T$ =0} \Comment{Update $\sigma^2$}
			\State $r\gets 1+(A/T - 0.574)^3 \cdot 1000$
			\State $A\gets 0$
			\If{$r>1.1$}
				\State $\sigma^2 \gets 1.1\sigma^2$
			\ElsIf{$r<0.9$}
				\State $\sigma^2 \gets 0.9\sigma^2$
			\Else
				\State $\sigma^2 \gets r \sigma^2$
			\EndIf
		\EndIf
		\EndFor
	\end{algorithmic}
	\hspace*{\algorithmicindent} \textbf{Output} $\{ \bm{\mu}_0, \bm{\mu}_1, \cdots, \bm{\mu}_N \}$
\end{algorithm}

The C++ code is available on \href{https://github.com/mkbwang/BIOSTAT882/blob/master/HW1/P5.cpp}{Github}. Following the instruction that $n=100$, $Y\sim N(10, 1)$ and both $\mu_1$ and $\mu_2$ are both equal to 5, I ran MALA for 40000 iterations with the scaling factor updated every 100 iterations(see this \href{https://github.com/mkbwang/BIOSTAT882/blob/master/HW1/P5.R}{R code}). The initial scaling factor is $10^{-8}$. Figure \ref{p5acceptance} indicates that MALA helps the acceptance rate converge to the ideal value of 0.574 quickly. 

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{/home/wangmk/UM/BIOSTAT882/HW1/P5acceptance.eps}
	\caption{Acceptance rate quickly converged around ideal value of 0.574 with MALA}\label{p5acceptance}
\end{figure}

The mixing of $\mu_1$ is not so good judging from Figure \ref{p5mixing}. The mixing of $\mu_2$ is better. According to the posterior distribution, $\mu_1$ has a mean value of 9.58 and a standard deviation of 0.17. $\mu_2$ has a mean value of 0.009 and a standard deviation of 0.032.

\begin{figure}[htbp]
	\begin{subfigure}[b]{0.45\columnwidth}
		\includegraphics[width=\linewidth]{tracemu1.eps}
		\caption{Trace of $\mu_1$ for the last 5000 iterations}
		\label{tracemu1}
	\end{subfigure}
	\hfill 
	\begin{subfigure}[b]{0.45\columnwidth}
		\includegraphics[width=\linewidth]{histmu1.eps}
		\caption{Distribution of $\mu_1$ for the last 5000 iterations}
		\label{histmu1}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\columnwidth}
		\includegraphics[width=\linewidth]{mu2trace.eps}
		\caption{Trace of $\mu_2$ for the last 5000 iterations}
		\label{tracemu2}
	\end{subfigure}
	\hfill 
	\begin{subfigure}[b]{0.45\columnwidth}
		\includegraphics[width=\linewidth]{histmu2.eps}
		\caption{Distribution of $\mu_2$ for the last 5000 iterations}
		\label{histmu2}
	\end{subfigure}
	
	\caption{Posterior Distribution of $\mu_1$ and $\mu_2$}\label{p5mixing}
\end{figure}
\end{document}
